<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache SkyWalking – Performance</title>
    <link>/tags/performance/</link>
    <description>Recent content in Performance on Apache SkyWalking</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 24 Jan 2022 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="/tags/performance/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Blog: Scaling with Apache SkyWalking</title>
      <link>/blog/2022-01-24-scaling-with-apache-skywalking/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-01-24-scaling-with-apache-skywalking/</guid>
      <description>
        
        
        &lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In the Apache SkyWalking ecosystem, the OAP obtains metrics, traces, logs, and event data through SkyWalking Agent, Envoy, or other data sources. Under the gRPC protocol, it transmits data by communicating with a single server node. Only when the connection is broken, the reconnecting policy would be used based on DNS round-robin mode. When new services are added at runtime or the OAP load is kept high due to increased traffic of observed services, the OAP cluster needs to scale out for increased traffic. The load of the new OAP node would be less due to all existing agents having connected to previous nodes. Even without scaling, the load of OAP nodes would be unbalanced, because the agent would keep the connection due to random policy at the booting stage. In these cases, it would become a challenge to keep up the health status of all nodes, and be able to scale out when needed.&lt;/p&gt;
&lt;p&gt;In this article, we mainly discuss how to solve this challenge in SkyWalking.&lt;/p&gt;
&lt;h2 id=&#34;how-to-load-balance&#34;&gt;How to Load Balance&lt;/h2&gt;
&lt;p&gt;SkyWalking mainly uses the gRPC protocol for data transmission, so this article mainly introduces load balancing in the gRPC protocol.&lt;/p&gt;
&lt;h3 id=&#34;proxy-or-client-side&#34;&gt;Proxy Or Client-side&lt;/h3&gt;
&lt;p&gt;Based on the &lt;a href=&#34;https://grpc.io/blog/grpc-load-balancing/&#34;&gt;gRPC official Load Balancing blog&lt;/a&gt;, there are two approaches to load balancing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Client-side&lt;/strong&gt;: The client perceives multiple back-end services and uses a load-balancing algorithm to select a back-end service for each RPC.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proxy&lt;/strong&gt;: The client sends the message to the proxy server, and the proxy server load balances the message to the back-end service.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From the perspective of observability system architecture:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Pros&lt;/th&gt;
&lt;th&gt;Cons&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Client-side&lt;/td&gt;
&lt;td&gt;&lt;li&gt;High performance because of the elimination of extra hop&lt;/li&gt;&lt;/td&gt;
&lt;td&gt;&lt;li&gt;Complex client (cluster awareness, load balancing, health check, etc.)&lt;/li&gt;&lt;li&gt;Ensure each data source to be connected provides complex client capabilities&lt;/li&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Proxy&lt;/td&gt;
&lt;td&gt;&lt;li&gt;Simple Client&lt;/li&gt;&lt;/td&gt;
&lt;td&gt;&lt;li&gt;Higher latency&lt;/li&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We choose Proxy mode for the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Observable data is not very time-sensitive, a little latency caused by transmission is acceptable. A little extra hop is acceptable and there is no impact on the client-side.&lt;/li&gt;
&lt;li&gt;As an observability platform, we cannot/should not ask clients to change. They make their own tech decisions and may have their own commercial considerations.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;transmission-policy&#34;&gt;Transmission Policy&lt;/h3&gt;
&lt;p&gt;In the proxy mode, we should determine the transmission path between downstream and upstream.&lt;/p&gt;
&lt;p&gt;Different data protocols require different processing policies. There are two transmission policies:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Synchronous&lt;/strong&gt;: Suitable for protocols that require data exchange in the client, such as SkyWalking Dynamic Configuration Service. This type of protocol provides real-time results.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Asynchronous batch&lt;/strong&gt;: Used when the client doesn’t care about the upstream processing results, but only the transmitted data (e.g., trace report, log report, etc.)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The synchronization policy requires that the proxy send the message to the upstream server when receiving the client message, and synchronously return the response data to the downstream client. Usually, only a few protocols need to use the synchronization policy.&lt;/p&gt;
&lt;p&gt;As shown below, after the client sends the request to the Proxy, the proxy would send the message to the server synchronously. When the proxy receives the result, it returns to the client.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./transmission-policy-synchronous.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The asynchronous batch policy means that the data is sent to the upstream server in batches asynchronously. This policy is more common because most protocols in SkyWalking are primarily based on data reporting. We think using the queue as a buffer could have a good effect. The asynchronous batch policy is executed according to the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The proxy receives the data and wraps it as an Event object.&lt;/li&gt;
&lt;li&gt;An event is added into the queue.&lt;/li&gt;
&lt;li&gt;When the cycle time is reached or when the queue elements reach the fixed number, the elements in the queue will parallel consume and send to the OAP.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The advantage of using queues is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Separate data receiving and sending to reduce the mutual influence.&lt;/li&gt;
&lt;li&gt;The interval quantization mechanism can be used to combine events, which helps to speed up sending events to the OAP.&lt;/li&gt;
&lt;li&gt;Using multi-threaded consumption queue events can make fuller use of network IO.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As shown below, after the proxy receives the message, the proxy would wrap the message as an event and push it to the queue. The message sender would take batch events from the queue and send them to the upstream OAP.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./transmission-policy-asynchronous.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;routing&#34;&gt;Routing&lt;/h3&gt;
&lt;p&gt;Routing algorithms are used to route messages to a single upstream server node.&lt;/p&gt;
&lt;p&gt;The Round-Robin algorithm selects nodes in order from the list of upstream service nodes. The advantage of this algorithm is that the number of times each node is selected is average. When the size of the data is close to the same, each upstream node can handle the same quantity of data content.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./routing-round-robin.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;With the Weight Round-Robin, each upstream server node has a corresponding routing weight ratio. The difference from Round-Robin is that each upstream node has more chances to be routed according to its weight. This algorithm is more suitable to use when the upstream server node machine configuration is not the same.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./routing-weight-round-robin.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Fixed algorithm is a hybrid algorithm. It can ensure that the same data is routed to the same upstream server node, and when the upstream server scales out, it still maintains routing to the same node; unless the upstream node does not exist, it will reroute. This algorithm is mainly used in the SkyWalking Meter protocol because this protocol needs to ensure that the metrics of the same service instance are sent to the same OAP node.
The Routing steps are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generate a unique identification string based on the data content, as short as possible. The amount of data is controllable.&lt;/li&gt;
&lt;li&gt;Get the upstream node of identity from LRU Cache, and use it if it exists.&lt;/li&gt;
&lt;li&gt;According to the identification, generate the corresponding hash value, and find the upstream server node from the upstream list.&lt;/li&gt;
&lt;li&gt;Save the mapping relationship between the upstream server node and identification to LRU Cache.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The advantage of this algorithm is to bind the data with the upstream server node as much as possible, so the upstream server can better process continuous data. The disadvantage is that it takes up a certain amount of memory space to save the corresponding relationship.&lt;/p&gt;
&lt;p&gt;As shown below, the image is divided into two parts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The left side represents that the same data content always is routed to the same server node.&lt;/li&gt;
&lt;li&gt;The right side represents the data routing algorithm. Get the number from the data, and use the remainder algorithm to obtain the position.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;./routing-fixed.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We choose to use a combination of Round-Robin and Fixed algorithm for routing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Fixed routing algorithm is suitable for specific protocols, mainly used when passing metrics data to the SkyWalking Meter protocol&lt;/li&gt;
&lt;li&gt;The Round-Robin algorithm is used by default. When the SkyWalking OAP cluster is deployed, the configuration of the nodes needs to be as much the same as possible, so there would be no need to use the Weight Round-Robin algorithm.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;how-to-balance-the-load-balancer-itself&#34;&gt;How to balance the load balancer itself?&lt;/h2&gt;
&lt;p&gt;Proxy still needs to deal with the load balancing problem from client to itself, especially when deploying a Proxy cluster in a production environment.&lt;/p&gt;
&lt;p&gt;There are three ways to solve this problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Connection management&lt;/strong&gt;: Use the &lt;code&gt;max_connection*&lt;/code&gt; config on the client-side to specify the maximum connection duration of each connection. For more information, please read the proposal.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cluster awareness&lt;/strong&gt;: The proxy has cluster awareness, and actively disconnects the connection when the load is unbalanced to allow the client to re-pick up the proxy.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resource limit+HPA&lt;/strong&gt;: Restrict the connection resource situation of each proxy, and no longer accept new connections when the resource limit is reached. And use the HPA mechanism of Kubernetes to dynamically scale out the number of the proxy.&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Connection management&lt;/th&gt;
&lt;th&gt;Cluster awareness&lt;/th&gt;
&lt;th&gt;Resource Limit+HPA&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Pros&lt;/td&gt;
&lt;td&gt;&lt;li&gt;Simple to use&lt;/li&gt;&lt;/td&gt;
&lt;td&gt;&lt;li&gt;Ensure that the number of connections in each proxy is relatively &lt;/li&gt;&lt;/td&gt;
&lt;td&gt;&lt;li&gt;Simple to use&lt;/li&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cons&lt;/td&gt;
&lt;td&gt;&lt;li&gt;Each client needs to ensure that data is not lost&lt;/li&gt;&lt;li&gt;The client is required to accept GOWAY responses&lt;/li&gt;&lt;/td&gt;
&lt;td&gt;&lt;li&gt;May cause a sudden increase in traffic on some nodes&lt;/li&gt;&lt;li&gt;Each client needs to ensure that data is not lost &lt;/li&gt;&lt;/td&gt;
&lt;td&gt;&lt;li&gt;Traffic will not be particularly balanced in each instance&lt;/li&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We choose Limit+HPA for these reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Easy to config and use the proxy and easy to understand based on basic data metrics.&lt;/li&gt;
&lt;li&gt;No data loss due to broken connection. There is no need for the client to implement any other protocols to prevent data loss, especially when the client is a commercial product.&lt;/li&gt;
&lt;li&gt;The connection of each node in the proxy cluster does not need to be particularly balanced, as long as the proxy node itself is high-performance.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;skywalking-satellite&#34;&gt;SkyWalking-Satellite&lt;/h2&gt;
&lt;p&gt;We have implemented this Proxy in the &lt;a href=&#34;https://github.com/apache/skywalking-satellite&#34;&gt;SkyWalking-Satellite&lt;/a&gt; project. It’s used between Client and SkyWalking OAP, effectively solving the load balancing problem.&lt;/p&gt;
&lt;p&gt;After the system is deployed, the Satellite would accept the traffic from the Client, and the Satellite will perceive all the nodes of the OAP through Kubernetes Label Selector or manual configuration, and load balance the traffic to the upstream OAP node.&lt;/p&gt;
&lt;p&gt;As shown below, a single client still maintains a connection with a single Satellite, Satellite would establish the connection with each OAP, and load balance message to the OAP node.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./skywalking-satellites.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;When scaling Satellite, we need to deploy the &lt;a href=&#34;https://github.com/apache/skywalking-swck&#34;&gt;SWCK&lt;/a&gt; adapter and configure the HPA in Kubernetes. SWCK is a platform for the SkyWalking users, provisions, upgrades, maintains SkyWalking relevant components, and makes them work natively on Kubernetes.&lt;/p&gt;
&lt;p&gt;After deployment is finished, the following steps would be performed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Read metrics from OAP&lt;/strong&gt;: HPA requests the SWCK metrics adapter to dynamically read the metrics in the OAP.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scaling the Satellite&lt;/strong&gt;: Kubernetes HPA senses that the metrics values are in line with expectations, so the Satellite would be scaling automatically.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As shown below, use the dotted line to divide the two parts. HPA uses SWCK Adapter to read the metrics in the OAP. When the threshold is met, HPA would scale the Satellite deployment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;swck-hpa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;In this section, we will demonstrate two cases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SkyWalking Scaling: After SkyWalking OAP scaling, the traffic would auto load balancing through Satellite.&lt;/li&gt;
&lt;li&gt;Satellite Scaling: Satellite’s own traffic load balancing.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;NOTE: All commands could be accessed through &lt;a href=&#34;https://github.com/mrproliu/sw-satellite-demo-scripts/tree/1180c23e8f3bb36778307f9ae15395274ca039b3&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;skywalking-scaling&#34;&gt;SkyWalking Scaling&lt;/h3&gt;
&lt;p&gt;We will use the &lt;a href=&#34;https://istio.io/latest/docs/examples/bookinfo/&#34;&gt;bookinfo application&lt;/a&gt; to demonstrate how to integrate Apache SkyWalking 8.9.1 with Apache SkyWalking-Satellite 0.5.0+, and observe the service mesh through the Envoy ALS protocol.&lt;/p&gt;
&lt;p&gt;Before starting, please make sure that you already have a Kubernetes environment.&lt;/p&gt;
&lt;h4 id=&#34;install-istio&#34;&gt;Install Istio&lt;/h4&gt;
&lt;p&gt;Istio provides a very convenient way to configure the Envoy proxy and enable the access log service. The following step:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install the istioctl locally to help manage the Istio mesh.&lt;/li&gt;
&lt;li&gt;Install Istio into the Kubernetes environment with a demo configuration profile, and enable the Envoy ALS. Transmit the ALS message to the satellite. The satellite we will deploy later.&lt;/li&gt;
&lt;li&gt;Add the label into the default namespace so Istio could automatically inject Envoy sidecar proxies when you deploy your application later.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#998;font-style:italic&#34;&gt;# install istioctl&lt;/span&gt;
&lt;span style=&#34;color:#0086b3&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#008080&#34;&gt;ISTIO_VERSION&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;1.12.0
curl -L https://istio.io/downloadIstio | sh - 
sudo mv &lt;span style=&#34;color:#008080&#34;&gt;$PWD&lt;/span&gt;/istio-&lt;span style=&#34;color:#008080&#34;&gt;$ISTIO_VERSION&lt;/span&gt;/bin/istioctl /usr/local/bin/

&lt;span style=&#34;color:#998;font-style:italic&#34;&gt;# install istio&lt;/span&gt;
istioctl install -y --set &lt;span style=&#34;color:#008080&#34;&gt;profile&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;demo &lt;span style=&#34;color:#d14&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#d14&#34;&gt;&lt;/span&gt;	--set meshConfig.enableEnvoyAccessLogService&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#0086b3&#34;&gt;true&lt;/span&gt; &lt;span style=&#34;color:#d14&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#d14&#34;&gt;&lt;/span&gt;	--set meshConfig.defaultConfig.envoyAccessLogService.address&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;skywalking-system-satellite.skywalking-system:11800

&lt;span style=&#34;color:#998;font-style:italic&#34;&gt;# enbale envoy proxy in default namespace&lt;/span&gt;
kubectl label namespace default istio-injection&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;enabled
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;install-swck&#34;&gt;Install SWCK&lt;/h4&gt;
&lt;p&gt;SWCK provides convenience for users to deploy and upgrade SkyWalking related components based on Kubernetes. The automatic scale function of Satellite also mainly relies on SWCK. For more information, you could refer to the &lt;a href=&#34;https://github.com/apache/skywalking-swck/blob/master/docs/operator.md#guides-of-operator-deployment&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#998;font-style:italic&#34;&gt;# Install cert-manager&lt;/span&gt;
kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml

&lt;span style=&#34;color:#998;font-style:italic&#34;&gt;# Deploy SWCK&lt;/span&gt;
mkdir -p skywalking-swck &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#0086b3&#34;&gt;cd&lt;/span&gt; skywalking-swck
wget https://dlcdn.apache.org/skywalking/swck/0.6.1/skywalking-swck-0.6.1-bin.tgz
tar -zxvf skywalking-swck-0.6.1-bin.tgz
&lt;span style=&#34;color:#0086b3&#34;&gt;cd&lt;/span&gt; config
kubectl apply -f operator-bundle.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;deploy-apache-skywalking-and-apache-skywalking-satellite&#34;&gt;Deploy Apache SkyWalking And Apache SkyWalking-Satellite&lt;/h4&gt;
&lt;p&gt;We have provided a simple script to deploy the skywalking OAP, UI, and Satellite.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#998;font-style:italic&#34;&gt;# Create the skywalking components namespace&lt;/span&gt;
kubectl create namespace skywalking-system
kubectl label namespace skywalking-system swck-injection&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;enabled
&lt;span style=&#34;color:#998;font-style:italic&#34;&gt;# Deploy components&lt;/span&gt;
kubectl apply -f https://raw.githubusercontent.com/mrproliu/sw-satellite-demo-scripts/5821a909b647f7c8f99c70378e197630836f45f7/resources/sw-components.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;deploy-bookinfo-application&#34;&gt;Deploy Bookinfo Application&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#0086b3&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#008080&#34;&gt;ISTIO_VERSION&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;1.12.0
kubectl apply -f https://raw.githubusercontent.com/istio/istio/&lt;span style=&#34;color:#008080&#34;&gt;$ISTIO_VERSION&lt;/span&gt;/samples/bookinfo/platform/kube/bookinfo.yaml
kubectl &lt;span style=&#34;color:#0086b3&#34;&gt;wait&lt;/span&gt; --for&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#008080&#34;&gt;condition&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;Ready pods --all --timeout&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;1200s
kubectl port-forward service/productpage &lt;span style=&#34;color:#099&#34;&gt;9080&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, please open your browser and visit &lt;code&gt;http://localhost:9080&lt;/code&gt;. You should be able to see the Bookinfo application. Refresh the webpage several times to generate enough access logs.&lt;/p&gt;
&lt;p&gt;Then, you can see the topology and metrics of the Bookinfo application on SkyWalking WebUI. At this time, you can see that the Satellite is working!&lt;/p&gt;
&lt;h4 id=&#34;deploy-monitor&#34;&gt;Deploy Monitor&lt;/h4&gt;
&lt;p&gt;We need to install OpenTelemetry Collector to collect metrics in OAPs and analyze them.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#998;font-style:italic&#34;&gt;# Add OTEL collector&lt;/span&gt;
kubectl apply -f https://raw.githubusercontent.com/mrproliu/sw-satellite-demo-scripts/5821a909b647f7c8f99c70378e197630836f45f7/resources/otel-collector-oap.yaml

kubectl port-forward -n skywalking-system  service/skywalking-system-ui 8080:80
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, please open your browser and visit &lt;code&gt;http://localhost:8080/&lt;/code&gt; and create a new item on the dashboard. The SkyWalking Web UI pictured below shows how the data content is applied.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./mesh-count-conf.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;scaling-oap&#34;&gt;Scaling OAP&lt;/h4&gt;
&lt;p&gt;Scaling the number of OAPs by deployment.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl scale --replicas&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#099&#34;&gt;3&lt;/span&gt; -n skywalking-system deployment/skywalking-system-oap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;done&#34;&gt;Done!&lt;/h4&gt;
&lt;p&gt;After a period of time, you will see that the number of OAPs becomes 3, and the ALS traffic is balanced to each OAP.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./mesh-count-list.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;satellite-scaling&#34;&gt;Satellite Scaling&lt;/h3&gt;
&lt;p&gt;After we have completed the SkyWalking Scaling, we would carry out the Satellite Scaling demo.&lt;/p&gt;
&lt;h4 id=&#34;deploy-swck-hpa&#34;&gt;Deploy SWCK HPA&lt;/h4&gt;
&lt;p&gt;SWCK provides an adapter to implement the Kubernetes external metrics to adapt the HPA through reading the metrics in SkyWalking OAP. We expose the metrics service in Satellite to OAP and configure HPA Resource to auto-scaling the Satellite.&lt;/p&gt;
&lt;p&gt;Install the SWCK adapter into the Kubernetes environment:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f skywalking-swck/config/adapter-bundle.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Create the HPA resource, and limit each Satellite to handle a maximum of 10 connections:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/mrproliu/sw-satellite-demo-scripts/5821a909b647f7c8f99c70378e197630836f45f7/resources/satellite-hpa.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, you could see we have 9 connections in one satellite. One envoy proxy may establish multiple connections to the satellite.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kubectl get HorizontalPodAutoscaler -n skywalking-system
NAME       REFERENCE                                TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/skywalking-system-satellite   9/10      &lt;span style=&#34;color:#099&#34;&gt;1&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;3&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;1&lt;/span&gt;          5m18s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;scaling-application&#34;&gt;Scaling Application&lt;/h4&gt;
&lt;p&gt;The scaling application could establish more connections to the satellite, to verify whether the HPA is in effect.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl scale --replicas&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#099&#34;&gt;3&lt;/span&gt; deployment/productpage-v1 deployment/details-v1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;done-1&#34;&gt;Done!&lt;/h4&gt;
&lt;p&gt;By default, Satellite will deploy a single instance and a single instance will only accept 11 connections. HPA resources limit one Satellite to handle 10 connections and use a stabilization window to make Satellite stable scaling up. In this case, we deploy the Bookinfo application in 10+ instances after scaling, which means that 10+ connections will be established to the Satellite.&lt;/p&gt;
&lt;p&gt;So after HPA resources are running, the Satellite would be automatically scaled up to 2 instances. You can learn about the calculation algorithm of replicas through the &lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details&#34;&gt;official documentation&lt;/a&gt;. Run the following command to view the running status:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kubectl get HorizontalPodAutoscaler -n skywalking-system --watch
NAME       REFERENCE                                TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/skywalking-system-satellite   11/10     &lt;span style=&#34;color:#099&#34;&gt;1&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;3&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;1&lt;/span&gt;          3m31s
hpa-demo   Deployment/skywalking-system-satellite   11/10     &lt;span style=&#34;color:#099&#34;&gt;1&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;3&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;1&lt;/span&gt;          4m20s
hpa-demo   Deployment/skywalking-system-satellite   11/10     &lt;span style=&#34;color:#099&#34;&gt;1&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;3&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;2&lt;/span&gt;          4m38s
hpa-demo   Deployment/skywalking-system-satellite   11/10     &lt;span style=&#34;color:#099&#34;&gt;1&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;3&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;2&lt;/span&gt;          5m8s
hpa-demo   Deployment/skywalking-system-satellite   6/10      &lt;span style=&#34;color:#099&#34;&gt;1&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;3&lt;/span&gt;         &lt;span style=&#34;color:#099&#34;&gt;2&lt;/span&gt;          5m23s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;By observing the “number of connections” metric, we would be able to see that when the number of connections of each gRPC exceeds 10 connections, then the satellite automatically scales through the HPA rule. As a result, the connection number is down to normal status (in this example, less than 10)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;swctl metrics linear --name satellite_service_grpc_connect_count --service-name satellite::satellite-service
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;satellite-connection-metrics.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: SkyWalking performance in Service Mesh scenario</title>
      <link>/blog/2019-01-25-mesh-loadtest/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-01-25-mesh-loadtest/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;Author: Hongtao Gao, Apache SkyWalking &amp;amp; ShardingShpere PMC&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hanahmily&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/hanahmily&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/gao-hongtao-47b835168/&#34;&gt;Linkedin&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Service mesh receiver was first introduced in Apache SkyWalking 6.0.0-beta. It is designed to provide a common entrance for receiving telemetry data from service mesh framework, for instance, Istio, Linkerd, Envoy etc. What’s the service mesh? According to Istio’s explain:&lt;/p&gt;
&lt;p&gt;The term service mesh is used to describe the network of microservices that make up such applications and the interactions between them.&lt;/p&gt;
&lt;p&gt;As a PMC member of Apache SkyWalking, I tested trace receiver and well understood the performance of collectors in trace scenario. I also would like to figure out the performance of service mesh receiver.&lt;/p&gt;
&lt;h2 id=&#34;different-between-trace-and-service-mesh&#34;&gt;Different between trace and service mesh&lt;/h2&gt;
&lt;p&gt;Following chart presents a typical trace map:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;You could find a variety of elements in it just like web service, local method, database, cache, MQ and so on. But service mesh only collect service network telemetry data that contains the entrance and exit data of a service for now(more elements will be imported soon, just like Database). A smaller quantity of data is sent to the service mesh receiver than the trace.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;But using sidecar is a little different.The client requesting “A” that will send a segment to service mesh receiver from “A”’s sidecar. If “A” depends on “B”,  another segment will be sent from “A”’s sidecar. But for a trace system, only one segment is received by the collector. The sidecar model splits one segment into small segments, that will increase service mesh receiver network overhead.&lt;/p&gt;
&lt;h2 id=&#34;deployment-architecture&#34;&gt;Deployment Architecture&lt;/h2&gt;
&lt;p&gt;In this test, I will pick two different backend deployment. One is called mini unit, consist of one collector and one elasticsearch instance. Another is a standard production cluster, contains three collectors and three elasticsearch instances.&lt;/p&gt;
&lt;p&gt;Mini unit is a suitable architecture for dev or test environment. It saves your time and VM resources, speeds up depolyment process.&lt;/p&gt;
&lt;p&gt;The standard cluster provides good performance and HA for a production scenario. Though you will pay more money and take care of the cluster carefully, the reliability of the cluster will be a good reward to you.&lt;/p&gt;
&lt;p&gt;I pick 8 CPU and 16GB VM to set up the test environment. This test targets the performance of normal usage scenarios, so that choice is reasonable. The cluster is built on Google Kubernetes Engine(GKE), and every node links each other with a VPC network. For running collector is a CPU intensive task, the resource request of collector deployment should be 8 CPU, which means every collector instance occupy a VM node.&lt;/p&gt;
&lt;h2 id=&#34;testing-process&#34;&gt;Testing Process&lt;/h2&gt;
&lt;p&gt;Receiving mesh fragments per second(MPS) depends on the following variables.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ingress query per second(QPS)&lt;/li&gt;
&lt;li&gt;The topology of a microservice cluster&lt;/li&gt;
&lt;li&gt;Service mesh mode(proxy or sidecar)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this test, I use Bookinfo app as a demo cluster.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image6.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;So every request will touch max 4 nodes. Plus picking the sidecar mode(every request will send two telemetry data),  the MPS will be QPS * 4 *2.&lt;/p&gt;
&lt;p&gt;There are also some important metrics that should be explained&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Client Query Latency: GraphQL API query response time heatmap.&lt;/li&gt;
&lt;li&gt;Client Mesh Sender: Send mesh segments per second. The total line represents total send amount and the error line is the total number of failed send.&lt;/li&gt;
&lt;li&gt;Mesh telemetry latency: service mesh receiver handling data heatmap.&lt;/li&gt;
&lt;li&gt;Mesh telemetry received: received mesh telemetry data per second.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mini-unit&#34;&gt;Mini Unit&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;image3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;You could find collector can process up to &lt;strong&gt;25k&lt;/strong&gt; data per second. The CPU usage is about 4 cores. Most of the query latency is less than 50ms. After login the VM on which collector instance running, I know that system load is reaching the limit(max is 8).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;According to the previous formula, a single collector instance could process &lt;strong&gt;3k&lt;/strong&gt; QPS of Bookinfo traffic.&lt;/p&gt;
&lt;h3 id=&#34;standard-cluster&#34;&gt;Standard Cluster&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;image4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Compare to the mini-unit, cluster’s throughput increases linearly. Three instances provide total 80k per second processing power. Query latency increases slightly, but it’s also very small(less than 500ms). I also checked every collector instance system load that all reached the limit. 10k QPS of BookInfo telemetry data could be processed by the cluster.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Let’s wrap them up. There are some important things you could get from this test.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;QPS varies by the there variables. The test results in this blog are not important. The user should pick property value according to his system.&lt;/li&gt;
&lt;li&gt;Collector cluster’s processing power could scale out.&lt;/li&gt;
&lt;li&gt;The collector is CPU intensive application. So you should provide sufficient CPU resource to it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This blog gives people a common method to evaluate the throughput of Service Mesh Receiver. Users could use this to design their Apache Skywalking backend deployment architecture.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
